{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a16097-6e56-4257-a3ae-1bfdb6a219ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to scrape...\n",
      "CSV file save to: C:/Users/dayining/Desktop/book_Project/data/book.csv\n",
      "scrape No.1 page: http://books.toscrape.com/\n",
      "  find 20 book\n",
      "scrape No.2 page: http://books.toscrape.com/catalogue/page-2.html\n",
      "  find 20 book\n",
      "scrape No.3 page: http://books.toscrape.com/catalogue/page-3.html\n",
      "  find 20 book\n",
      "scrape No.4 page: http://books.toscrape.com/catalogue/page-4.html\n",
      "  find 20 book\n",
      "scrape No.5 page: http://books.toscrape.com/catalogue/page-5.html\n",
      "  find 20 book\n",
      "done 5 page\n",
      "\n",
      "Total find 100 books, from 5 pages\n",
      "dealing with book 10/100\n",
      "dealing with book 20/100\n",
      "dealing with book 30/100\n",
      "dealing with book 40/100\n",
      "dealing with book 50/100\n",
      "dealing with book 60/100\n",
      "dealing with book 70/100\n",
      "dealing with book 80/100\n",
      "dealing with book 90/100\n",
      "dealing with book 100/100\n",
      "succeed scrape 100 book date!\n",
      "file save to: C:/Users/dayining/Desktop/book_Project/data/book.csv\n",
      "\n",
      "Category summary:\n",
      "Sequential Art: 14 book\n",
      "Nonfiction: 12 book\n",
      "Default: 9 book\n",
      "Poetry: 7 book\n",
      "Fiction: 5 book\n",
      "Food and Drink: 5 book\n",
      "Add a comment: 5 book\n",
      "History: 4 book\n",
      "Young Adult: 4 book\n",
      "Fantasy: 4 book\n",
      "Mystery: 3 book\n",
      "Music: 3 book\n",
      "Thriller: 3 book\n",
      "Childrens: 3 book\n",
      "Science Fiction: 2 book\n",
      "Romance: 2 book\n",
      "Spirituality: 2 book\n",
      "Philosophy: 2 book\n",
      "Historical Fiction: 1 book\n",
      "Business: 1 book\n",
      "Politics: 1 book\n",
      "Travel: 1 book\n",
      "Art: 1 book\n",
      "New Adult: 1 book\n",
      "Contemporary: 1 book\n",
      "Science: 1 book\n",
      "Health: 1 book\n",
      "Horror: 1 book\n",
      "Self Help: 1 book\n",
      "Done! Timing: 166.95秒\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "def extract_genre(breadcrumb):\n",
    "    \"\"\"\n",
    "    直接提取面包屑导航中的第三级类目(extract categories from detail pages)\n",
    "    \n",
    "    参数(parameter):\n",
    "        breadcrumb: 包含分类路径的面包屑元素(elements in category path)\n",
    "    \n",
    "    返回(return):\n",
    "        str: 三级类目名称(third categories names)\n",
    "    \"\"\"\n",
    "    # Extract all breadcrumb items (excluding the book title in the last item)\n",
    "    # 提取所有面包屑项（排除最后一项的书名）\n",
    "    items = [li.text.strip() for li in breadcrumb.select('li')][:-1]\n",
    "\n",
    "    # Ensure there are sufficient levels\n",
    "    # 确保有足够的层级\n",
    "    if len(items) >= 3:\n",
    "        return items[2]  # 第三项就是三级类目(third item is the aimed category)\n",
    "    elif len(items) == 2:\n",
    "        return items[1]  # 只有二级类目的情况(if only has secondary category)\n",
    "    return \"Uncategorized\"\n",
    "\n",
    "\n",
    "def clean_price(price_str):\n",
    "    \"\"\"清理并格式化价格字符串\"\"\"\n",
    "    \"\"\"Clean and format the price string\"\"\"\n",
    "\n",
    "    # Confirm the string clean\n",
    "    # 确保价格字符串是干净的\n",
    "    price_number = ''.join(char for char in price_str if char.isdigit() or char == '.')\n",
    "    return f\"￡{price_number}\"  # 使用全角的￡符号(Use the full-width \"￡\" symbol)\n",
    "\n",
    "\n",
    "def scrape_books():\n",
    "    # 设置明确的CSV保存路径\n",
    "    # set up CSV save path\n",
    "    \n",
    "    save_path = os.path.join(os.getcwd(), 'books.csv')\n",
    "    print(f\"CSV file save to: {'C:/Users/dayining/Desktop/book_Project/data/book.csv'}\")\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    base_url = \"http://books.toscrape.com/\"\n",
    "    current_page = base_url\n",
    "    all_book_links = []\n",
    "    page_count = 0\n",
    "    max_pages = 5  # 设置最大爬取页数（set maximum number of pages to scrape）\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # 循环爬取前5页\n",
    "        # Loop to scrape the first 5 pages\n",
    "        while current_page and page_count < max_pages:\n",
    "            page_count += 1\n",
    "            print(f\"scrape No.{page_count} page: {current_page}\")\n",
    "            \n",
    "            # 获取当前页内容\n",
    "            # scrape for current page\n",
    "            \n",
    "            response = requests.get(current_page, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 获取当前页的所有书籍链接\n",
    "            # scrape for current page link\n",
    "            \n",
    "            page_book_links = []\n",
    "            for article in soup.select('article.product_pod'):\n",
    "                link = article.select_one('h3 a')['href']\n",
    "                # 处理相对URL\n",
    "                # deal with relative URLs\n",
    "                full_url = urljoin(current_page, link)\n",
    "                page_book_links.append(full_url)\n",
    "            \n",
    "            print(f\"  find {len(page_book_links)} book\")\n",
    "            all_book_links.extend(page_book_links)\n",
    "            \n",
    "            # 查找下一页链接\n",
    "            # search for next page link\n",
    "            next_button = soup.select_one('li.next a')\n",
    "            if next_button and page_count < max_pages:\n",
    "                next_url = next_button['href']\n",
    "                current_page = urljoin(current_page, next_url)\n",
    "            else:\n",
    "                current_page = None\n",
    "                print(f\"done {max_pages} page\")\n",
    "                \n",
    "        total_books = len(all_book_links)  \n",
    "        print(f\"\\nTotal find {len(all_book_links)} books, from {page_count} pages\")\n",
    "        \n",
    "        \n",
    "        books_data = []\n",
    "        \n",
    "        \n",
    "        # 遍历每本书的详情页\n",
    "        # Through each book's detail page\n",
    "        for i, book_url in enumerate(all_book_links, 1):\n",
    "            try:\n",
    "                # 显示简洁进度信息\n",
    "                # Display concise progress information\n",
    "                if i % 10 == 0 or i == total_books:\n",
    "                    print(f\"dealing with book {i}/{total_books}\")\n",
    "                \n",
    "                book_response = requests.get(book_url, headers=headers)\n",
    "                book_response.raise_for_status()\n",
    "                \n",
    "                book_soup = BeautifulSoup(book_response.text, 'html.parser')\n",
    "                \n",
    "                # 提取书籍信息\n",
    "                # Book information\n",
    "                title = book_soup.select_one('h1').text.strip()\n",
    "                \n",
    "                # 提取价格并处理\n",
    "                # Price information\n",
    "                price_raw = book_soup.select_one('p.price_color').text.strip()\n",
    "                price = clean_price(price_raw)\n",
    "                \n",
    "                # 提取评分\n",
    "                # Rating information\n",
    "                rating_class = book_soup.select_one('p.star-rating')['class']\n",
    "                rating = rating_class[1] if len(rating_class) > 1 else \"Not rated\"\n",
    "                \n",
    "                # 提取作者\n",
    "                # Author information\n",
    "                author_element = book_soup.find('th', string='Author')\n",
    "                author = author_element.find_next_sibling('td').text.strip() if author_element else \"Unknown\"\n",
    "                \n",
    "                # 提取分类信息（三级类目）\n",
    "                # Third categories information\n",
    "                breadcrumb = book_soup.select_one('.breadcrumb')\n",
    "                genre = \"Uncategorized\"  \n",
    "\n",
    "                if breadcrumb:\n",
    "                    # 打印面包屑项用于调试\n",
    "                    # Print breadcrumb for test\n",
    "                    breadcrumb_items = [li.text.strip() for li in breadcrumb.select('li')]\n",
    "                    #print(f\"Breadcrumb path items: {breadcrumb_items}\")\n",
    "                    \n",
    "                    # 直接提取第三项作为三级类目\n",
    "                    # Directly extract the third item as the tertiary category\n",
    "                    if len(breadcrumb_items) >= 3:\n",
    "                        genre = breadcrumb_items[2]\n",
    "                        #print(f\"third category: {genre}\")\n",
    "                    else:\n",
    "                        print(f\"lack of breadcrumb levels: {len(breadcrumb_items)}\")\n",
    "                else:\n",
    "                    print(\"not found breadcrumb\")\n",
    "                \n",
    "                # 提取出版年份\n",
    "                # Publication date\n",
    "                date_element = book_soup.find('th', string='Publication date')\n",
    "                pub_date = date_element.find_next_sibling('td').text.strip() if date_element else \"Unknown\"\n",
    "                \n",
    "                # 提取年份\n",
    "                # Year\n",
    "                year_match = re.search(r'\\d{4}', pub_date)\n",
    "                year = year_match.group(0) if year_match else \"Unknown\"\n",
    "                \n",
    "                # 提取库存状态作为流行度指标\n",
    "                # Stock for popularity\n",
    "                stock_element = book_soup.find('th', string='Availability')\n",
    "                availability = stock_element.find_next_sibling('td').text.strip() if stock_element else \"Unknown\"\n",
    "                # 将库存状态转换为数值\n",
    "                # stock for number\n",
    "                stock_match = re.search(r'(\\d+)', availability)\n",
    "                stock_count = int(stock_match.group(1)) if stock_match else 0\n",
    "                \n",
    "                # 提取产品描述\n",
    "                # Description\n",
    "                description_element = book_soup.select_one('#product_description + p')\n",
    "                description = description_element.text.strip() if description_element else \"No description available\"\n",
    "                \n",
    "                # 添加到书籍数据列表\n",
    "                # Add to the book data list\n",
    "                books_data.append([\n",
    "                    title, author, genre, year, \n",
    "                    stock_count, rating, price, \n",
    "                    description, book_url\n",
    "                ])\n",
    "                \n",
    "                \n",
    "                time.sleep(0.5)\n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"deal with {book_url} error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 保存到CSV\n",
    "        # Save to CSV\n",
    "        with open('C:/Users/dayining/Desktop/book_Project/data/book.csv', 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # 添加标题行\n",
    "            # Add column headers\n",
    "            writer.writerow([\n",
    "                'Title', 'Author', 'Genre', 'Publication Year', \n",
    "                'Popularity (Stock)', 'Rating', 'Price', \n",
    "                'Description', 'URL'\n",
    "            ])\n",
    "            writer.writerows(books_data)\n",
    "            \n",
    "        print(f\"succeed scrape {len(books_data)} book date!\")\n",
    "        print(f\"file save to: {'C:/Users/dayining/Desktop/book_Project/data/book.csv'}\")\n",
    "        \n",
    "        # 打印分类统计\n",
    "        # Print categories sum\n",
    "        genres = [row[2] for row in books_data]\n",
    "        print(\"\\nCategory summary:\")\n",
    "        genre_counts = {}\n",
    "        for g in genres:\n",
    "            genre_counts[g] = genre_counts.get(g, 0) + 1\n",
    "        \n",
    "        # 按书籍数量排序\n",
    "        # Ranking by count\n",
    "        sorted_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for genre, count in sorted_genres:\n",
    "            print(f\"{genre}: {count} book\")\n",
    "        \n",
    "        return True, total_books  \n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"network error: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"scrape error: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"start to scrape...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    success, total_books = scrape_books()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    \n",
    "    if success:\n",
    "        print(f\"Done! Timing: {elapsed:.2f}秒\")\n",
    "    else:\n",
    "        print(\"Failed, please check error message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af15fd7-c4f6-4ea3-900d-12add3927dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book Project",
   "language": "python",
   "name": "book-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
